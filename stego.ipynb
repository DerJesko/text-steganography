{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch import nn\n",
    "from math import pow\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "from bitstring import BitArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_mes = bytes(input(\"Enter the message you want to hide. Only use ASCII characters\"),\"ascii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_len = 16\n",
    "#keygen\n",
    "key = get_random_bytes(key_len)\n",
    "cipher = AES.new(key, AES.MODE_GCM)\n",
    "#encrypt\n",
    "ciphertext, tag = cipher.encrypt_and_digest(secret_mes)\n",
    "encode_this = BitArray(cipher.nonce + ciphertext + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitArray('0xe565a7a911662998c8c2e4d333ced161b7a1fd698b5e6f2d19e8e57bcf5ea576b09f435d11c1dee60187dfe8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257])\n",
      "0.5000057220458984 49494 torch.Size([49494]) 0\n",
      "lr index 763 50257\n",
      "0.5000150203704834 24746 torch.Size([24746]) 1\n",
      "lr index 25511 50257\n",
      "0.5000158548355103 12373 torch.Size([12373]) 2\n",
      "lr index 37884 50257\n",
      "0.5000550746917725 6187 torch.Size([6187]) 3\n",
      "lr index 37884 44071\n",
      "0.5000677108764648 3094 torch.Size([3094]) 4\n",
      "lr index 37884 40978\n",
      "0.5 1547 torch.Size([1547]) 5\n",
      "lr index 39431 40978\n",
      "0.5003232359886169 774 torch.Size([774]) 6\n",
      "lr index 39431 40205\n",
      "0.5000001788139343 387 torch.Size([387]) 7\n",
      "lr index 39818 40205\n",
      "0.5012921690940857 194 torch.Size([194]) 8\n",
      "lr index 39818 40012\n",
      "0.5 97 torch.Size([97]) 9\n",
      "lr index 39915 40012\n",
      "0.5051546096801758 49 torch.Size([49]) 10\n",
      "lr index 39963 40012\n",
      "0.510204017162323 25 torch.Size([25]) 11\n",
      "lr index 39963 39988\n",
      "0.5199999213218689 13 torch.Size([13]) 12\n",
      "lr index 39963 39976\n",
      "0.5384615659713745 7 torch.Size([7]) 13\n",
      "lr index 39969 39976\n",
      "0.5714285969734192 4 torch.Size([4]) 14\n",
      "lr index 39969 39973\n",
      "0.5 2 torch.Size([2]) 15\n",
      "lr index 39971 39973\n",
      "0.5 1 torch.Size([1]) 16\n",
      "lr index 39972 39973\n"
     ]
    }
   ],
   "source": [
    "result = \"Dear audience,\"\n",
    "current_tokens = tokenizer(result, return_tensors=\"pt\")\n",
    "#print(current_tokens.input_ids)\n",
    "next_token_logits = model(**current_tokens).logits[:, -1, :]\n",
    "filtered_next_token_logits = torch.softmax(top_k_top_p_filtering(next_token_logits, top_p=0.999)[0],0)\n",
    "print(filtered_next_token_logits.shape)\n",
    "l_index = 0\n",
    "r_index = len(filtered_next_token_logits)\n",
    "j=0\n",
    "while len(filtered_next_token_logits) > 1:\n",
    "    b = encode_this[j]\n",
    "\n",
    "    acc = 0\n",
    "    i = 0\n",
    "    if not b:\n",
    "        while acc<0.5:\n",
    "            acc += filtered_next_token_logits[i]\n",
    "            i += 1\n",
    "        \n",
    "        r_index -= len(filtered_next_token_logits)-i\n",
    "        filtered_next_token_logits = torch.softmax(filtered_next_token_logits[:i],0)\n",
    "    else:\n",
    "        while acc<0.5:\n",
    "            acc += filtered_next_token_logits[-i]\n",
    "            i += 1\n",
    "\n",
    "        l_index += len(filtered_next_token_logits)-i\n",
    "        filtered_next_token_logits = torch.softmax(filtered_next_token_logits[-i:],0)\n",
    "    print(float(acc),i,filtered_next_token_logits.shape,j)\n",
    "    print(\"lr index\", l_index, r_index)\n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_next_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_string = \"Dear audience,\"\n",
    "previous_string = \"\"\n",
    "\n",
    "i = 0\n",
    "while i < len(encode_this):\n",
    "    b = encode_this[i]\n",
    "    print(\"Current string:\", current_string)\n",
    "    current_tokens = tokenizer(current_string, return_tensors=\"pt\")\n",
    "    print(current_tokens.input_ids)\n",
    "    probs = torch.topk(nn.functional.softmax(model(**current_tokens).logits[:, -1, :], dim=-1),100)\n",
    "    (indeces_0, indeces_1) = balance_probs(probs)\n",
    "\n",
    "    if b:\n",
    "        (indeces_right, indeces_wrong) = (indeces_1, indeces_0)\n",
    "    else:\n",
    "        (indeces_right, indeces_wrong) = (indeces_0, indeces_1)\n",
    "    \n",
    "    print(\"Don't use these tokens \", [tokenizer.decode(probs.indices[0,i]) for i in indeces_wrong])\n",
    "    print(\"Prefer these tokens \", [tokenizer.decode(probs.indices[0,i]) for i in indeces_right])\n",
    "    \n",
    "    new_token_string = input(\"add another token\")\n",
    "    new_token = tokenizer(new_token_string).input_ids[0]\n",
    "\n",
    "    print([probs.indices[0,i] for i in indeces_right], [probs.indices[0,i] for i in indeces_wrong])\n",
    "\n",
    "    if new_token in [probs.indices[0,i] for i in indeces_right]:\n",
    "        i += 1\n",
    "        print(i)\n",
    "    elif new_token in [probs.indices[0,i] for i in indeces_wrong]:\n",
    "        print(\"token from the 'don't use set'\")\n",
    "        continue\n",
    "\n",
    "    previous_string = current_string[:]\n",
    "    current_string += new_token_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_string = \"Dear audience, please do your own due and take\"\n",
    "seed_len = 3\n",
    "in_tokens = tokenizer(in_string, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = []\n",
    "\n",
    "for i in range(seed_len, in_tokens.size()[1]):\n",
    "    current_tokens = in_tokens[:,:i]\n",
    "    probs = torch.topk(nn.functional.softmax(model(current_tokens).logits[:,-1, :], dim=-1),8)\n",
    "    indeces = balance_probs(probs)\n",
    "    new_token = in_tokens[0,i]\n",
    "\n",
    "    if new_token in [probs.indices[0,i] for i in indeces[0]]:\n",
    "        bs.append(False)\n",
    "    elif new_token in [probs.indices[0,i] for i in indeces[1]]:\n",
    "        bs.append(True)\n",
    "\n",
    "decrypt_this = BitArray(bs).tobytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonce = decrypt_this[:key_len]\n",
    "ciphertext = decrypt_this[key_len:-key_len]\n",
    "tag = decrypt_this[-key_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrypt\n",
    "cipher = AES.new(key, AES.MODE_GCM, nonce)\n",
    "cipher.decrypt_and_verify(ciphertext, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
