{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from math import pow\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "from bitstring import BitArray"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "secret_mes = bytes(input(\"Enter the message you want to hide. Only use ASCII characters\"),\"ascii\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encryption"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "key_len = 16\n",
    "#keygen\n",
    "key = get_random_bytes(key_len)\n",
    "cipher = AES.new(key, AES.MODE_GCM)\n",
    "#encrypt\n",
    "ciphertext, tag = cipher.encrypt_and_digest(secret_mes)\n",
    "encode_this = BitArray(cipher.nonce + ciphertext + tag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper functions to create balanced probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def leq_set(k,l):\n",
    "    result = set([frozenset()])\n",
    "    for i in range(k):\n",
    "        new_result = result.copy()\n",
    "        #print(new_result)\n",
    "        for r in result:\n",
    "            for j in l:\n",
    "                new_r = r.union(frozenset([j]))\n",
    "                #print(new_r)\n",
    "                new_result.add(new_r)\n",
    "        result = new_result\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def set_combinations(l):\n",
    "    result = set()\n",
    "    l1 = leq_set(int(len(l)/2), l)\n",
    "    l1.remove(frozenset())\n",
    "    for i in l1:\n",
    "        ln = [j for j in l if not j in i]\n",
    "        l2 = leq_set(len(ln), ln)\n",
    "        l2.remove(frozenset())\n",
    "        for j in l2:\n",
    "            t = (i,j)\n",
    "            result.add(t)\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def balance_probs(probs):\n",
    "    length = probs.values.size()[1]\n",
    "    combs = set_combinations(range(length))\n",
    "    indeces = None\n",
    "    difference = 1\n",
    "    for (comb1,comb2) in combs:\n",
    "        sum1 = 0\n",
    "        for i in comb1:\n",
    "            sum1 += probs.values[0,i]\n",
    "        sum2 = 0\n",
    "        for i in comb2:\n",
    "            sum2 += probs.values[0,i]\n",
    "\n",
    "        a = torch.abs(sum1-sum2)\n",
    "        if a < difference:\n",
    "            difference = a\n",
    "            indeces = (comb1,comb2)\n",
    "    return indeces"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "current_string = \"Dear audience,\"\n",
    "previous_string = \"\"\n",
    "\n",
    "i = 0\n",
    "while i < len(encode_this):\n",
    "    b = encode_this[i]\n",
    "    print(\"Current string:\", current_string)\n",
    "    current_tokens = tokenizer(current_string, return_tensors=\"pt\")\n",
    "    print(current_tokens.input_ids)\n",
    "    probs = torch.topk(nn.functional.softmax(model(**current_tokens).logits[:, -1, :], dim=-1),8)\n",
    "    (indeces_0, indeces_1) = balance_probs(probs)\n",
    "\n",
    "    if b:\n",
    "        (indeces_right, indeces_wrong) = (indeces_1, indeces_0)\n",
    "    else:\n",
    "        (indeces_right, indeces_wrong) = (indeces_0, indeces_1)\n",
    "    \n",
    "    print(\"Don't use these tokens \", [tokenizer.decode(probs.indices[0,i]) for i in indeces_wrong])\n",
    "    print(\"Prefer these tokens \", [tokenizer.decode(probs.indices[0,i]) for i in indeces_right])\n",
    "    \n",
    "    new_token_string = input(\"add another token\")\n",
    "    new_token = tokenizer(new_token_string).input_ids[0]\n",
    "\n",
    "    print([probs.indices[0,i] for i in indeces_right], [probs.indices[0,i] for i in indeces_wrong])\n",
    "\n",
    "    if new_token in [probs.indices[0,i] for i in indeces_right]:\n",
    "        i += 1\n",
    "        print(i)\n",
    "    elif new_token in [probs.indices[0,i] for i in indeces_wrong]:\n",
    "        print(\"token from the 'don't use set'\")\n",
    "        continue\n",
    "\n",
    "    previous_string = current_string[:]\n",
    "    current_string += new_token_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "in_string = \"Dear audience, please do your own due and take\"\n",
    "seed_len = 3\n",
    "in_tokens = tokenizer(in_string, return_tensors=\"pt\").input_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bs = []\n",
    "\n",
    "for i in range(seed_len, in_tokens.size()[1]):\n",
    "    current_tokens = in_tokens[:,:i]\n",
    "    probs = torch.topk(nn.functional.softmax(model(current_tokens).logits[:,-1, :], dim=-1),8)\n",
    "    indeces = balance_probs(probs)\n",
    "    new_token = in_tokens[0,i]\n",
    "\n",
    "    if new_token in [probs.indices[0,i] for i in indeces[0]]:\n",
    "        bs.append(False)\n",
    "    elif new_token in [probs.indices[0,i] for i in indeces[1]]:\n",
    "        bs.append(True)\n",
    "\n",
    "decrypt_this = BitArray(bs).tobytes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decryption"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nonce = decrypt_this[:key_len]\n",
    "ciphertext = decrypt_this[key_len:-key_len]\n",
    "tag = decrypt_this[-key_len:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#decrypt\n",
    "cipher = AES.new(key, AES.MODE_GCM, nonce)\n",
    "cipher.decrypt_and_verify(ciphertext, tag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}