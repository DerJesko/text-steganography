{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch import nn\n",
    "from math import pow\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "from bitstring import BitArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_mes = bytes(input(\"Enter the message you want to hide. Only use ASCII characters\"),\"ascii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_len = 16\n",
    "#keygen\n",
    "key = get_random_bytes(key_len)\n",
    "cipher = AES.new(key, AES.MODE_GCM)\n",
    "#encrypt\n",
    "ciphertext, tag = cipher.encrypt_and_digest(secret_mes)\n",
    "encode_this = BitArray(cipher.nonce + ciphertext + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"Dear audience,\"\n",
    "current_tokens = tokenizer(result, return_tensors=\"pt\")\n",
    "#print(current_tokens.input_ids)\n",
    "next_token_logits = model(**current_tokens).logits[:, -1, :]\n",
    "filtered_next_token_logits = torch.softmax(top_k_top_p_filtering(next_token_logits, top_p=0.9)[0],0)\n",
    "print(filtered_next_token_logits.shape)\n",
    "l_index = 0\n",
    "r_index = len(filtered_next_token_logits)\n",
    "j=0\n",
    "while len(filtered_next_token_logits) > 1:\n",
    "    b = encode_this[j]\n",
    "\n",
    "    acc = 0\n",
    "    if not b:\n",
    "        i = 0\n",
    "        while acc<0.5:\n",
    "            acc += filtered_next_token_logits[i]\n",
    "            i += 1\n",
    "        i -= 1\n",
    "        \n",
    "        over = acc-0.5\n",
    "        filtered_next_token_logits[i] -= over\n",
    "        r_index -= len(filtered_next_token_logits)-i\n",
    "        filtered_next_token_logits = 2*filtered_next_token_logits[:i+1]\n",
    "    else:\n",
    "        i = -1\n",
    "        while acc<0.5:\n",
    "            acc += filtered_next_token_logits[i]\n",
    "            i -= 1\n",
    "        i += 1\n",
    "\n",
    "        over = acc-0.5\n",
    "        filtered_next_token_logits[i] -= over\n",
    "        l_index += len(filtered_next_token_logits)+i-1\n",
    "        filtered_next_token_logits = 2*filtered_next_token_logits[i:] # in the end replace this by a softmax, will be more stable\n",
    "    if j>20:\n",
    "        print(filtered_next_token_logits)\n",
    "    print(float(acc),i,filtered_next_token_logits.shape,j)\n",
    "    print(\"lr index\", l_index, r_index)\n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"a\",\"b\"]\n",
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_string = \"Dear audience,\"\n",
    "previous_string = \"\"\n",
    "\n",
    "i = 0\n",
    "while i < len(encode_this):\n",
    "    b = encode_this[i]\n",
    "    print(\"Current string:\", current_string)\n",
    "    current_tokens = tokenizer(current_string, return_tensors=\"pt\")\n",
    "    print(current_tokens.input_ids)\n",
    "    probs = torch.topk(nn.functional.softmax(model(**current_tokens).logits[:, -1, :], dim=-1),100)\n",
    "    (indeces_0, indeces_1) = balance_probs(probs)\n",
    "\n",
    "    if b:\n",
    "        (indeces_right, indeces_wrong) = (indeces_1, indeces_0)\n",
    "    else:\n",
    "        (indeces_right, indeces_wrong) = (indeces_0, indeces_1)\n",
    "    \n",
    "    print(\"Don't use these tokens \", [tokenizer.decode(probs.indices[0,i]) for i in indeces_wrong])\n",
    "    print(\"Prefer these tokens \", [tokenizer.decode(probs.indices[0,i]) for i in indeces_right])\n",
    "    \n",
    "    new_token_string = input(\"add another token\")\n",
    "    new_token = tokenizer(new_token_string).input_ids[0]\n",
    "\n",
    "    print([probs.indices[0,i] for i in indeces_right], [probs.indices[0,i] for i in indeces_wrong])\n",
    "\n",
    "    if new_token in [probs.indices[0,i] for i in indeces_right]:\n",
    "        i += 1\n",
    "        print(i)\n",
    "    elif new_token in [probs.indices[0,i] for i in indeces_wrong]:\n",
    "        print(\"token from the 'don't use set'\")\n",
    "        continue\n",
    "\n",
    "    previous_string = current_string[:]\n",
    "    current_string += new_token_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_string = \"Dear audience, please do your own due and take\"\n",
    "seed_len = 3\n",
    "in_tokens = tokenizer(in_string, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = []\n",
    "\n",
    "for i in range(seed_len, in_tokens.size()[1]):\n",
    "    current_tokens = in_tokens[:,:i]\n",
    "    probs = torch.topk(nn.functional.softmax(model(current_tokens).logits[:,-1, :], dim=-1),8)\n",
    "    indeces = balance_probs(probs)\n",
    "    new_token = in_tokens[0,i]\n",
    "\n",
    "    if new_token in [probs.indices[0,i] for i in indeces[0]]:\n",
    "        bs.append(False)\n",
    "    elif new_token in [probs.indices[0,i] for i in indeces[1]]:\n",
    "        bs.append(True)\n",
    "\n",
    "decrypt_this = BitArray(bs).tobytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonce = decrypt_this[:key_len]\n",
    "ciphertext = decrypt_this[key_len:-key_len]\n",
    "tag = decrypt_this[-key_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrypt\n",
    "cipher = AES.new(key, AES.MODE_GCM, nonce)\n",
    "cipher.decrypt_and_verify(ciphertext, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
